{
  "strategies": {
    "baseline": {
      "name": "baseline",
      "description": "Ultra-simple hate speech detection (V4 Optimal)",
      "system_prompt": "You classify text as 'hate' or 'normal'. Be sensitive to any mean, attacking, or discriminatory content.\n\nJSON format required: {\"classification\": \"hate\", \"rationale\": \"brief reason\"} or {\"classification\": \"normal\", \"rationale\": \"brief reason\"}",
      "user_template": "Text: \"{text}\"\n\nIs this hate speech? Answer 'hate' if it's mean, attacking, or discriminatory. Answer 'normal' if it's not.\n\nJSON: {{\"classification\": \"[hate/normal]\", \"rationale\": \"[why]\"}}",
      "parameters": {
        "max_tokens": 256,
        "temperature": 0.3,
        "top_p": 1.0,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "response_format": "json_object"
      }
    },
    "policy": {
      "name": "policy",
      "description": "Simple policy-based detection with examples (V4 Optimal)",
      "system_prompt": "You detect hate speech based on simple rules. Be sensitive to attacks on people.\n\nJSON format required: {\"classification\": \"hate\", \"rationale\": \"brief reason\"} or {\"classification\": \"normal\", \"rationale\": \"brief reason\"}\n\nHate speech includes: attacks on race, religion, gender, sexuality, slurs, stereotypes.",
      "user_template": "HATE SPEECH RULE: Text is hate speech if it attacks, insults, or stereotypes people based on who they are.\n\nExamples of hate speech:\n- Racial slurs or attacks\n- Religious mockery or threats  \n- Gender-based insults\n- Anti-LGBTQ+ language\n- Ethnic stereotypes\n\nText: \"{text}\"\n\nDoes this attack or insult people based on race, religion, gender, sexuality, or ethnicity?\n\nAnswer: 'hate' if yes, 'normal' if no.\n\nJSON: {{\"classification\": \"[hate/normal]\", \"rationale\": \"[explain briefly]\"}}",
      "parameters": {
        "max_tokens": 512,
        "temperature": 0.4,
        "top_p": 0.9,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "response_format": "json_object"
      }
    },
    "persona": {
      "name": "persona",
      "description": "Community harm detection with specific examples (V5 Optimal)",
      "system_prompt": "You check if text would hurt different communities. Be sensitive to any harmful content targeting specific groups.\n\nJSON format required: {\"classification\": \"hate\", \"rationale\": \"brief reason\"} or {\"classification\": \"normal\", \"rationale\": \"brief reason\"}\n\nExamples of harm to communities:\n- LGBTQ+: slurs, \"gay people are sick\", \"trans people are perverts\"\n- Hispanic/Latino: \"Mexicans are criminals\", \"all Latinos are lazy\", ethnic slurs\n- Middle Eastern: \"Muslims are terrorists\", \"Arabs are dangerous\", religious hatred\n- Any group: stereotypes, generalizations, dehumanizing language",
      "user_template": "Text: \"{text}\"\n\nQuick checks with examples:\n- Would this hurt LGBTQ+ people? (like anti-gay slurs, trans hatred)\n- Would this hurt Hispanic/Latino people? (like \"Mexicans are criminals\")\n- Would this hurt Middle Eastern people? (like \"Muslims are terrorists\")\n- Would this hurt any other group? (stereotypes, slurs, generalizations)\n\nIf ANY answer is 'yes', classify as 'hate'.\nIf ALL answers are 'no', classify as 'normal'.\n\nJSON: {{\"classification\": \"[hate/normal]\", \"rationale\": \"[which group would be hurt or why it's safe]\"}}",
      "parameters": {
        "max_tokens": 512,
        "temperature": 0.35,
        "top_p": 1.0,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "response_format": "json_object"
      }
    },
    "combined": {
      "name": "combined",
      "description": "Comprehensive detection with concrete examples (V5 Optimal)",
      "system_prompt": "You find hate speech using simple questions and concrete examples. Be sensitive to any harmful content.\n\nJSON format required: {\"classification\": \"hate\", \"rationale\": \"brief reason\"} or {\"classification\": \"normal\", \"rationale\": \"brief reason\"}\n\nExamples to watch for:\n- Attacks: \"All [group] are [negative trait]\"\n- Slurs: racial, religious, gender, sexuality slurs\n- Stereotypes: \"[Group] people always [negative behavior]\"\n- Threats: \"[Group] should be [harmful action]\"",
      "user_template": "Text: \"{text}\"\n\nSimple questions with examples:\n1. Does this attack someone's race, religion, gender, or sexuality? (like \"All [race] are criminals\")\n2. Does this use slurs or mean names for groups? (racial slurs, \"f*ggot\", etc.)\n3. Would this hurt LGBTQ+, Hispanic, or Middle Eastern people? (see examples above)\n4. Is this mean or discriminatory toward any group? (stereotypes, generalizations)\n\nIf you answer 'YES' to any question, classify as 'hate'.\nIf you answer 'NO' to all questions, classify as 'normal'.\n\nJSON: {{\"classification\": \"[hate/normal]\", \"rationale\": \"[which question was yes or confirm all no]\"}}",
      "parameters": {
        "max_tokens": 256,
        "temperature": 0.4,
        "top_p": 0.95,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "response_format": "json_object"
      }
    }
  }
}
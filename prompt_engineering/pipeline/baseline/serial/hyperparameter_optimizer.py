"""
Hyperparameter Optimization Pipeline for Hate Speech Detection

This module implements a comprehensive 3-phase hyperparameter optimization approach:
- Phase 1: Consistency Verification (Conservative & Standard variants)
- Phase 2: Performance Optimization (Balanced & Focused variants)  
- Phase 3: Recall Maximization (Creative & Exploratory variants)

The pipeline uses the existing evaluation metrics system with bias analysis
across LGBTQ+, Middle Eastern, and Mexican target groups.
"""

import logging
import json
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from datetime import datetime
import statistics
import copy

# Import existing evaluation system  
import sys
import os
# Add path to prompt_engineering directory (3 levels up: serial -> baseline -> pipeline -> prompt_engineering)
prompt_engineering_path = os.path.join(os.path.dirname(__file__), '..', '..', '..')
sys.path.append(prompt_engineering_path)

from metrics.evaluation_metrics_calc import EvaluationMetrics, PerformanceMetrics, BiasMetrics
from prompt_runner import PromptRunner
from loaders import load_dataset_by_filename, DatasetType


@dataclass
class OptimizationVariant:
    """
    Configuration for a specific optimization variant.
    """
    name: str
    phase: int
    description: str
    hyperparameters: Dict[str, Any]
    success_criteria: Dict[str, float]
    target_metrics: List[str]
    
    
@dataclass  
class PhaseResult:
    """
    Results from a completed optimization phase.
    """
    phase: int
    variants_tested: List[str]
    best_variant: str
    success_achieved: bool
    performance_metrics: Dict[str, PerformanceMetrics]
    bias_metrics: Dict[str, Dict[str, List[BiasMetrics]]]
    execution_time: float
    
    
@dataclass
class OptimizationResults:
    """
    Complete optimization run results.
    """
    run_id: str
    timestamp: str
    phases_completed: List[PhaseResult]
    optimal_configuration: Optional[OptimizationVariant]
    final_f1_score: float
    final_bias_balance: float
    total_execution_time: float
    recommendations: List[str]


class HyperparameterOptimizer:
    """
    Main hyperparameter optimization engine implementing the 3-phase approach.
    """
    
    def __init__(self, model_id: str = "gpt-oss-120b", 
                 config_path: Optional[str] = None,
                 output_dir: str = "outputs/optimization"):
        """
        Initialize the hyperparameter optimizer.
        
        Args:
            model_id: Model identifier for Azure AI
            config_path: Path to model configuration YAML
            output_dir: Directory for optimization results
        """
        self.logger = logging.getLogger(__name__)
        self.model_id = model_id
        self.config_path = config_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize evaluation system
        self.evaluator = EvaluationMetrics()
        
        # Storage for results
        self.baseline_results: Optional[Dict] = None
        self.phase_results: List[PhaseResult] = []
        
        # Generate run ID
        self.run_id = f"opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
    def initialize_baseline(self, data_source: str = "canned_100_stratified",
                          template_file: str = "baseline_v1.json") -> Dict:
        """
        Establish baseline performance using the default configuration.
        
        Args:
            data_source: Dataset to use for baseline measurement
            template_file: Baseline prompt template
            
        Returns:
            Dict: Baseline performance results
        """
        self.logger.info("[BASELINE] Establishing baseline performance...")
        
        try:
            runner = PromptRunner(
                model_id=self.model_id,
                config_path=self.config_path,
                prompt_template_file=template_file
            )
            
            # Run baseline validation
            results = runner.run_validation(
                strategies=["baseline_standard"],
                data_source=data_source,
                output_dir=str(self.output_dir / "baseline"),
                sample_size=None,
                use_concurrent=False,  # More consistent results
                max_workers=1,
                batch_size=5
            )
            
            self.baseline_results = results
            
            # Extract key metrics from performance CSV file
            baseline_summary = self._extract_metrics_from_run(results)
            
            # Store the extracted metrics for phase comparisons
            self.baseline_metrics = baseline_summary
            
            self.logger.info(f"[SUCCESS] Baseline established: F1={baseline_summary['f1_score']:.3f}")
            return baseline_summary
            
        except Exception as e:
            self.logger.error(f"[ERROR] Failed to establish baseline: {e}")
            raise
    
    def _extract_metrics_from_run(self, results: Dict[str, Any]) -> Dict[str, float]:
        """
        Extract performance metrics from run results by reading the performance metrics CSV file.
        
        Args:
            results: Results dictionary from prompt_runner
            
        Returns:
            Dict[str, float]: Dictionary with f1_score, precision, recall, accuracy
        """
        import pandas as pd
        import os
        from pathlib import Path
        
        default_metrics = {'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0, 'accuracy': 0.0}
        
        try:
            # Get the output directory from results
            output_dir = results.get('output_directory', '')
            if not output_dir:
                self.logger.warning("No output directory in results")
                return default_metrics
            
            # Look for performance_metrics CSV file
            run_dir = Path(output_dir)
            metrics_files = list(run_dir.glob('performance_metrics_*.csv'))
            
            if not metrics_files:
                self.logger.warning(f"No performance metrics file found in {run_dir}")
                return default_metrics
            
            # Read the performance metrics CSV
            metrics_file = metrics_files[0]  # Take the first one if multiple exist
            df = pd.read_csv(metrics_file)
            
            if df.empty:
                self.logger.warning(f"Empty metrics file: {metrics_file}")
                return default_metrics
            
            # Extract metrics from the first row (should be only one row for single strategy)
            metrics = {}
            metrics['f1_score'] = float(df['f1_score'].iloc[0]) if 'f1_score' in df.columns else 0.0
            metrics['precision'] = float(df['precision'].iloc[0]) if 'precision' in df.columns else 0.0
            metrics['recall'] = float(df['recall'].iloc[0]) if 'recall' in df.columns else 0.0
            metrics['accuracy'] = float(df['accuracy'].iloc[0]) if 'accuracy' in df.columns else 0.0
            
            return metrics
            
        except Exception as e:
            self.logger.warning(f"Failed to extract metrics from CSV: {e}")
            return default_metrics
    
    def get_phase_variants(self) -> Dict[int, List[OptimizationVariant]]:
        """
        Define all optimization variants for the 3-phase approach.
        
        Returns:
            Dict: Variants organized by phase
        """
        
        # Base hyperparameters (will be modified for each variant)
        base_params = {
            "temperature": 0.7,
            "max_tokens": 150,
            "top_p": 0.9,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "batch_size": 5,
            "max_workers": 1
        }
        
        variants = {
            # Phase 1: Consistency Verification
            1: [
                OptimizationVariant(
                    name="conservative",
                    phase=1,
                    description="Conservative settings for consistent performance",
                    hyperparameters={**base_params, "temperature": 0.3, "top_p": 0.8},
                    success_criteria={"variance_threshold": 0.05, "baseline_maintenance": 0.95},
                    target_metrics=["consistency", "stability"]
                ),
                OptimizationVariant(
                    name="standard",
                    phase=1,
                    description="Standard balanced settings",
                    hyperparameters={**base_params, "temperature": 0.7, "top_p": 0.9},
                    success_criteria={"variance_threshold": 0.05, "baseline_maintenance": 1.0},
                    target_metrics=["consistency", "baseline_match"]
                )
            ],
            
            # Phase 2: Performance Optimization  
            2: [
                OptimizationVariant(
                    name="balanced",
                    phase=2,
                    description="Optimized for F1-score improvement",
                    hyperparameters={**base_params, "temperature": 0.8, "max_tokens": 200, 
                                   "frequency_penalty": 0.1},
                    success_criteria={"f1_improvement": 0.02},
                    target_metrics=["f1_score"]
                ),
                OptimizationVariant(
                    name="focused",
                    phase=2,
                    description="Optimized for precision improvement",
                    hyperparameters={**base_params, "temperature": 0.5, "top_p": 0.7,
                                   "presence_penalty": 0.2},
                    success_criteria={"precision_improvement": 0.05},
                    target_metrics=["precision"]
                )
            ],
            
            # Phase 3: Recall Maximization
            3: [
                OptimizationVariant(
                    name="creative",
                    phase=3,
                    description="Creative settings for recall improvement",
                    hyperparameters={**base_params, "temperature": 1.0, "top_p": 0.95,
                                   "max_tokens": 250},
                    success_criteria={"recall_improvement": 0.03},
                    target_metrics=["recall"]
                ),
                OptimizationVariant(
                    name="exploratory", 
                    phase=3,
                    description="Exploratory settings for pattern discovery",
                    hyperparameters={**base_params, "temperature": 1.2, "top_p": 1.0,
                                   "frequency_penalty": -0.1, "max_tokens": 300},
                    success_criteria={"pattern_discovery": 5},
                    target_metrics=["recall", "pattern_discovery"]
                )
            ]
        }
        
        return variants
    
    def run_variant_evaluation(self, variant: OptimizationVariant,
                             data_source: str = "canned_100_stratified",
                             template_file: str = "all_combined.json",
                             runs: int = 3) -> Dict:
        """
        Execute evaluation for a specific variant with multiple runs for consistency.
        
        Args:
            variant: Optimization variant to test
            data_source: Dataset for evaluation
            template_file: Template file to use
            runs: Number of runs for consistency measurement
            
        Returns:
            Dict: Evaluation results with consistency metrics
        """
        self.logger.info(f"ðŸ§ª Testing variant: {variant.name} (Phase {variant.phase})")
        
        run_results = []
        
        for run_idx in range(runs):
            try:
                # Create a temporary template with modified parameters
                # This would integrate with the template system to modify hyperparameters
                runner = PromptRunner(
                    model_id=self.model_id,
                    config_path=self.config_path,
                    prompt_template_file=template_file
                )
                
                # Apply hyperparameters (this would need integration with the template system)
                # For now, we'll use the existing parameters and focus on the evaluation framework
                
                results = runner.run_validation(
                    strategies=["baseline", "policy", "persona", "combined"],
                    data_source=data_source,
                    output_dir=str(self.output_dir / f"{variant.name}_run_{run_idx}"),
                    sample_size=None,
                    use_concurrent=False,
                    max_workers=variant.hyperparameters.get("max_workers", 1),
                    batch_size=variant.hyperparameters.get("batch_size", 5)
                )
                
                run_results.append(results)
                
            except Exception as e:
                self.logger.error(f"[ERROR] Run {run_idx} failed for variant {variant.name}: {e}")
                continue
        
        if not run_results:
            raise ValueError(f"All runs failed for variant {variant.name}")
        
        # Calculate consistency metrics
        consistency_results = self._calculate_consistency_metrics(run_results, variant)
        
        return consistency_results
    
    def _calculate_consistency_metrics(self, run_results: List[Dict], 
                                     variant: OptimizationVariant) -> Dict:
        """
        Calculate consistency and variance metrics across multiple runs.
        
        Args:
            run_results: Results from multiple runs
            variant: Variant configuration
            
        Returns:
            Dict: Consistency analysis results
        """
        
        # Extract F1 scores from all runs using CSV files
        f1_scores = []
        precision_scores = []
        recall_scores = []
        
        for result in run_results:
            metrics = self._extract_metrics_from_run(result)
            f1_scores.append(metrics['f1_score'])
            precision_scores.append(metrics['precision'])
            recall_scores.append(metrics['recall'])
        
        # Calculate variance and mean
        f1_mean = statistics.mean(f1_scores) if f1_scores else 0.0
        f1_variance = statistics.variance(f1_scores) if len(f1_scores) > 1 else 0.0
        f1_std = statistics.stdev(f1_scores) if len(f1_scores) > 1 else 0.0
        
        precision_mean = statistics.mean(precision_scores) if precision_scores else 0.0
        recall_mean = statistics.mean(recall_scores) if recall_scores else 0.0
        
        # Calculate relative variance (coefficient of variation)
        variance_coefficient = (f1_std / f1_mean) if f1_mean > 0 else float('inf')
        
        consistency_result = {
            'variant_name': variant.name,
            'variant_phase': variant.phase,
            'runs_completed': len(run_results),
            'f1_scores': f1_scores,
            'f1_mean': f1_mean,
            'f1_variance': f1_variance,
            'f1_std': f1_std,
            'variance_coefficient': variance_coefficient,
            'precision_mean': precision_mean,
            'recall_mean': recall_mean,
            'consistency_score': 1.0 - min(1.0, variance_coefficient),  # Higher is better
            'best_run_result': max(run_results, key=lambda x: self._extract_metrics_from_run(x)['f1_score'])
        }
        
        return consistency_result
    
    def evaluate_phase_success(self, phase: int, variant_results: Dict[str, Dict]) -> PhaseResult:
        """
        Evaluate if a phase has met its success criteria.
        
        Args:
            phase: Phase number (1, 2, or 3)
            variant_results: Results from all variants in the phase
            
        Returns:
            PhaseResult: Phase evaluation results
        """
        self.logger.info(f"[EVALUATION] Evaluating Phase {phase} success criteria...")
        
        variants = self.get_phase_variants()[phase]
        success_achieved = False
        best_variant = None
        best_score = 0.0
        
        # Phase-specific success evaluation
        if phase == 1:  # Consistency Verification
            for variant in variants:
                result = variant_results.get(variant.name, {})
                variance_coeff = result.get('variance_coefficient', float('inf'))
                f1_mean = result.get('f1_mean', 0.0)
                
                baseline_f1 = self.baseline_results.get('f1_score', 0.0) if self.baseline_results else 0.0
                baseline_maintenance = (f1_mean / baseline_f1) if baseline_f1 > 0 else 0.0
                
                # Check success criteria
                variance_ok = variance_coeff < variant.success_criteria.get('variance_threshold', 0.05)
                baseline_ok = baseline_maintenance >= variant.success_criteria.get('baseline_maintenance', 0.95)
                
                if variance_ok and baseline_ok:
                    success_achieved = True
                    consistency_score = result.get('consistency_score', 0.0)
                    if consistency_score > best_score:
                        best_score = consistency_score
                        best_variant = variant.name
        
        elif phase == 2:  # Performance Optimization
            baseline_f1 = self.baseline_results.get('f1_score', 0.0) if self.baseline_results else 0.0
            baseline_precision = self.baseline_results.get('precision', 0.0) if self.baseline_results else 0.0
            
            for variant in variants:
                result = variant_results.get(variant.name, {})
                f1_mean = result.get('f1_mean', 0.0)
                precision_mean = result.get('precision_mean', 0.0)
                
                # Calculate improvements
                f1_improvement = (f1_mean - baseline_f1) / baseline_f1 if baseline_f1 > 0 else 0.0
                precision_improvement = (precision_mean - baseline_precision) / baseline_precision if baseline_precision > 0 else 0.0
                
                # Check success criteria
                if variant.name == "balanced" and f1_improvement >= variant.success_criteria.get('f1_improvement', 0.02):
                    success_achieved = True
                    if f1_mean > best_score:
                        best_score = f1_mean
                        best_variant = variant.name
                
                elif variant.name == "focused" and precision_improvement >= variant.success_criteria.get('precision_improvement', 0.05):
                    success_achieved = True
                    if precision_mean > best_score:
                        best_score = precision_mean
                        best_variant = variant.name
        
        elif phase == 3:  # Recall Maximization
            baseline_recall = self.baseline_results.get('recall', 0.0) if self.baseline_results else 0.0
            
            for variant in variants:
                result = variant_results.get(variant.name, {})
                recall_mean = result.get('recall_mean', 0.0)
                
                # Calculate recall improvement
                recall_improvement = (recall_mean - baseline_recall) / baseline_recall if baseline_recall > 0 else 0.0
                
                # Check success criteria
                if recall_improvement >= variant.success_criteria.get('recall_improvement', 0.03):
                    success_achieved = True
                    if recall_mean > best_score:
                        best_score = recall_mean
                        best_variant = variant.name
        
        # Create phase result
        phase_result = PhaseResult(
            phase=phase,
            variants_tested=[v.name for v in variants],
            best_variant=best_variant or "none",
            success_achieved=success_achieved,
            performance_metrics={},  # Would be populated with full metrics
            bias_metrics={},  # Would be populated with bias analysis
            execution_time=0.0  # Would track actual execution time
        )
        
        return phase_result
    
    def run_optimization(self, data_source: str = "canned_100_stratified") -> OptimizationResults:
        """
        Execute the complete 3-phase optimization pipeline.
        
        Args:
            data_source: Dataset for optimization
            
        Returns:
            OptimizationResults: Complete optimization results
        """
        start_time = datetime.now()
        self.logger.info("[START] Starting hyperparameter optimization pipeline...")
        
        try:
            # Step 1: Establish baseline
            self.initialize_baseline(data_source)
            
            # Step 2: Execute phases
            all_variants = self.get_phase_variants()
            
            for phase_num in [1, 2, 3]:
                self.logger.info(f"[PHASE] Starting Phase {phase_num}")
                phase_variants = all_variants[phase_num]
                variant_results = {}
                
                # Test each variant in the phase
                for variant in phase_variants:
                    try:
                        result = self.run_variant_evaluation(variant, data_source)
                        variant_results[variant.name] = result
                    except Exception as e:
                        self.logger.error(f"[ERROR] Variant {variant.name} failed: {e}")
                        continue
                
                # Evaluate phase success
                phase_result = self.evaluate_phase_success(phase_num, variant_results)
                self.phase_results.append(phase_result)
                
                # Log phase results
                if phase_result.success_achieved:
                    self.logger.info(f"[SUCCESS] Phase {phase_num} SUCCESS: Best variant = {phase_result.best_variant}")
                else:
                    self.logger.warning(f"[WARNING] Phase {phase_num} did not meet success criteria")
            
            # Step 3: Select optimal configuration
            optimal_config = self._select_optimal_configuration()
            
            # Step 4: Final validation
            final_metrics = self._final_validation(optimal_config, data_source)
            
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            # Create final results
            results = OptimizationResults(
                run_id=self.run_id,
                timestamp=start_time.isoformat(),
                phases_completed=self.phase_results,
                optimal_configuration=optimal_config,
                final_f1_score=final_metrics.get('f1_score', 0.0),
                final_bias_balance=final_metrics.get('bias_balance', 0.0),
                total_execution_time=execution_time,
                recommendations=self._generate_recommendations()
            )
            
            # Save results
            self._save_optimization_results(results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"[ERROR] Optimization pipeline failed: {e}")
            raise
    
    def _select_optimal_configuration(self) -> Optional[OptimizationVariant]:
        """
        Select the best performing configuration across all phases.
        
        Returns:
            OptimizationVariant: Optimal configuration or None if no good option found
        """
        # Implementation would analyze all phase results and select the best overall configuration
        # For now, return a placeholder
        return OptimizationVariant(
            name="optimal",
            phase=0,
            description="Selected optimal configuration",
            hyperparameters={},
            success_criteria={},
            target_metrics=[]
        )
    
    def _final_validation(self, config: Optional[OptimizationVariant], 
                         data_source: str) -> Dict:
        """
        Perform final validation of the optimal configuration.
        
        Args:
            config: Optimal configuration to validate
            data_source: Dataset for validation
            
        Returns:
            Dict: Final validation metrics
        """
        # Implementation would run comprehensive validation including bias analysis
        return {
            'f1_score': 0.85,  # Placeholder
            'bias_balance': 0.95  # Placeholder
        }
    
    def _generate_recommendations(self) -> List[str]:
        """
        Generate optimization recommendations based on results.
        
        Returns:
            List[str]: List of recommendations
        """
        recommendations = []
        
        # Analyze results and generate recommendations
        successful_phases = [p for p in self.phase_results if p.success_achieved]
        
        if len(successful_phases) == 3:
            recommendations.append("[SUCCESS] All optimization phases completed successfully")
        elif len(successful_phases) >= 2:
            recommendations.append("[WARNING] Most optimization phases succeeded - review failed phases")
        else:
            recommendations.append("[ERROR] Consider adjusting success criteria or hyperparameter ranges")
        
        recommendations.append("[MONITORING] Continue monitoring bias metrics across target groups")
        recommendations.append("[TESTING] Consider A/B testing the optimal configuration in production")
        
        return recommendations
    
    def _save_optimization_results(self, results: OptimizationResults) -> None:
        """
        Save optimization results to file.
        
        Args:
            results: Results to save
        """
        results_file = self.output_dir / f"optimization_results_{self.run_id}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(results), f, indent=2, default=str)
        
        self.logger.info(f"[SAVE] Optimization results saved to: {results_file}")


def create_hyperparameter_optimizer(model_id: str = "gpt-oss-120b", 
                                  config_path: Optional[str] = None) -> HyperparameterOptimizer:
    """
    Factory function to create a hyperparameter optimizer.
    
    Args:
        model_id: Model identifier
        config_path: Path to model configuration
        
    Returns:
        HyperparameterOptimizer: Configured optimizer instance
    """
    return HyperparameterOptimizer(model_id=model_id, config_path=config_path)
# Unified Accelerate Approach for Inference + Fine-tuning

## Why Use Accelerate for Both?

**YES! This is the cleanest and most systematic approach!** ‚úÖ

---

## Current Situation

### What We Have Now:
```
Inference:   Multi-process (CUDA_VISIBLE_DEVICES) ‚úÖ Works but manual
Fine-tuning: Not implemented ‚ùå
```

### Problem with Current Approach:
- ‚ùå Two different multi-GPU strategies
- ‚ùå Manual dataset splitting for inference
- ‚ùå Different code paths for inference vs training
- ‚ùå More code to maintain

---

## Proposed: Unified Accelerate Approach

### What We'll Have:
```
Inference:   Accelerate (automatic multi-GPU) ‚úÖ
Fine-tuning: Accelerate (automatic multi-GPU) ‚úÖ
```

### Benefits:
- ‚úÖ **One library** for both inference and fine-tuning
- ‚úÖ **Automatic** dataset splitting and distribution
- ‚úÖ **Same code** for single-GPU and multi-GPU
- ‚úÖ **Cleaner** and more maintainable
- ‚úÖ **Future-proof** for distributed training across nodes
- ‚úÖ **Zero changes** to switch between 1, 2, or 4 GPUs

---

## Performance Comparison

| Approach | Inference Speedup | Fine-tuning Speedup | Code Complexity |
|----------|-------------------|---------------------|-----------------|
| **Current (Multi-process)** | 4x ‚úÖ | N/A | High (manual splits) |
| **Accelerate (Unified)** | 3.8x ‚úÖ | 3.5x ‚úÖ | **Low (automatic!)** |

**Trade-off**: Slightly lower speedup (3.8x vs 4x) but **much cleaner code** and unified approach!

---

## Implementation Plan

### Phase 1: Add Accelerate to Inference (This Week)

**Benefits**:
- Convert existing runner to use Accelerate
- Automatic data distribution
- No manual dataset splitting needed
- Same command for 1 or 4 GPUs

**Example**:
```python
# Before (manual multi-process):
CUDA_VISIBLE_DEVICES=0 python runner.py --data_source gpu0_split &
CUDA_VISIBLE_DEVICES=1 python runner.py --data_source gpu1_split &
# ... wait and merge results

# After (Accelerate automatic):
accelerate launch --num_processes 4 runner.py --data_source unified
# Done! Accelerate handles everything!
```

---

### Phase 2: Add Fine-tuning with Same Accelerate (Next Week)

**Benefits**:
- Same Accelerate wrapper
- Same multi-GPU logic
- Consistent codebase

---

## Detailed Implementation

### Step 1: Install and Configure Accelerate

```bash
# Install
pip install accelerate

# Configure (one-time setup)
accelerate config

# Interactive prompts:
# ‚úì This machine
# ‚úì No distributed training (multi-GPU on single machine)
# ‚úì NO (not using DeepSpeed)
# ‚úì NO (not using Megatron-LM)
# ‚úì 4 (number of GPUs)
# ‚úì NO (not using FSDP)
# ‚úì bf16 (mixed precision)
# ‚úì (default for other options)
```

This creates: `~/.cache/huggingface/accelerate/default_config.yaml`

---

### Step 2: Modify `runner.py` to Support Accelerate

**Option A: Minimal Changes (Wrap Existing Code)**

Add accelerate wrapper around existing `run_baseline()`:

```python
# At top of runner.py
from accelerate import Accelerator

def run_baseline_with_accelerate(args):
    """Wrapper for multi-GPU inference with Accelerate."""
    
    # Initialize accelerator
    accelerator = Accelerator()
    
    # Only main process prints
    if accelerator.is_main_process:
        print(f"\n{'='*60}")
        print("BASELINE VALIDATION (Multi-GPU with Accelerate)")
        print(f"{'='*60}")
        print(f"Using {accelerator.num_processes} GPUs")
        print(f"Model: {args.model_name}")
        print(f"Data Source: {args.data_source}")
        print(f"{'='*60}\n")
    
    # Load model
    connector = LocalModelConnector(args.model_name, args.cache_dir, batch_size=args.batch_size)
    connector.load_model_once()
    
    # Wrap model with Accelerate (distributes across GPUs)
    connector.model = accelerator.prepare(connector.model)
    
    # Load full dataset (Accelerate will split automatically!)
    dataset = load_validation_data(args.data_source, args.max_samples)
    
    # Split dataset across processes (automatic!)
    with accelerator.split_between_processes(dataset) as process_dataset:
        # Each GPU processes its subset
        results = []
        for sample in tqdm(process_dataset, disable=not accelerator.is_local_main_process):
            result = process_sample(connector, strategy, sample, args.strategy, args.data_source)
            results.append(result)
    
    # Gather results from all GPUs
    all_results = accelerator.gather_for_metrics(results)
    
    # Only main process saves results
    if accelerator.is_main_process:
        # Calculate metrics and save (existing code)
        evaluator = EvaluationMetrics()
        metrics = evaluator.calculate_comprehensive_metrics(...)
        # ... save reports
    
    return 0
```

**Key Changes**:
1. ‚úÖ Wrap model with `accelerator.prepare()`
2. ‚úÖ Use `split_between_processes()` to auto-distribute data
3. ‚úÖ Use `gather_for_metrics()` to collect results
4. ‚úÖ Only main process saves files

**No manual dataset splitting needed!** üéâ

---

**Option B: Full Refactor (Cleaner)**

Create new `runner_accelerate.py` that uses Accelerate throughout:

```python
#!/usr/bin/env python3
"""
Unified runner using Accelerate for both inference and fine-tuning.
"""

from accelerate import Accelerator
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from pathlib import Path
from tqdm import tqdm

class AcceleratedRunner:
    """Unified runner for inference and fine-tuning with Accelerate."""
    
    def __init__(self, args):
        self.args = args
        self.accelerator = Accelerator(
            mixed_precision='bf16',
            gradient_accumulation_steps=1,
        )
        
    def load_model(self):
        """Load model and tokenizer."""
        model = AutoModelForCausalLM.from_pretrained(
            self.args.model_name,
            torch_dtype=torch.bfloat16,
            device_map={'': self.accelerator.device},  # Place on current device
        )
        tokenizer = AutoTokenizer.from_pretrained(self.args.model_name)
        
        # Prepare model for multi-GPU
        self.model = self.accelerator.prepare(model)
        self.tokenizer = tokenizer
        
        if self.accelerator.is_main_process:
            print(f"[OK] Model loaded on {self.accelerator.num_processes} GPUs")
    
    def run_inference(self):
        """Run baseline inference with automatic multi-GPU."""
        
        # Load dataset (full dataset, Accelerate will split)
        dataset = load_validation_data(self.args.data_source, self.args.max_samples)
        
        if self.accelerator.is_main_process:
            print(f"[OK] Loaded {len(dataset)} samples")
            print(f"Each GPU processes ~{len(dataset)//self.accelerator.num_processes} samples")
        
        # Process with automatic data parallelism
        results = []
        with self.accelerator.split_between_processes(dataset) as my_samples:
            for sample in tqdm(
                my_samples, 
                desc=f"GPU {self.accelerator.process_index}",
                disable=not self.accelerator.is_local_main_process
            ):
                result = self._process_sample(sample)
                results.append(result)
        
        # Gather all results
        all_results = self.accelerator.gather_for_metrics(results)
        
        # Main process handles metrics and saving
        if self.accelerator.is_main_process:
            self._save_results(all_results)
        
        return 0
    
    def run_finetuning(self):
        """Run fine-tuning with automatic multi-GPU (future)."""
        # To be implemented
        pass
    
    def _process_sample(self, sample):
        """Process single sample (inference)."""
        # Same logic as before, but model is already on correct device
        with torch.no_grad():
            # ... inference code
        return result
    
    def _save_results(self, results):
        """Save results (only called by main process)."""
        # Same as before
        pass


def main():
    parser = argparse.ArgumentParser()
    # ... same args as before
    args = parser.parse_args()
    
    runner = AcceleratedRunner(args)
    runner.load_model()
    
    if args.finetune_template:
        return runner.run_finetuning()
    else:
        return runner.run_inference()


if __name__ == "__main__":
    sys.exit(main())
```

**Launch**:
```bash
# Single GPU (automatic)
python runner_accelerate.py --data_source unified --max_samples 20

# Multi-GPU (automatic with accelerate!)
accelerate launch --num_processes 4 runner_accelerate.py --data_source unified
```

---

### Step 3: Add Fine-tuning (Same Pattern!)

```python
def run_finetuning(self):
    """Run fine-tuning with LoRA and multi-GPU."""
    
    # Load LoRA config
    from peft import LoraConfig, get_peft_model, TaskType
    
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        task_type=TaskType.CAUSAL_LM
    )
    
    # Add LoRA to model
    self.model = get_peft_model(self.model, lora_config)
    
    # Load training data
    train_dataset = load_jsonl(self.args.train_file)
    val_dataset = load_jsonl(self.args.val_file)
    
    # Prepare optimizer and dataloaders
    optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=4)
    
    # Prepare for multi-GPU (automatic!)
    self.model, optimizer, train_loader, val_loader = self.accelerator.prepare(
        self.model, optimizer, train_loader, val_loader
    )
    
    # Training loop
    for epoch in range(3):
        self.model.train()
        for batch in tqdm(train_loader, disable=not self.accelerator.is_local_main_process):
            outputs = self.model(**batch)
            loss = outputs.loss
            
            # Accelerate handles gradient synchronization!
            self.accelerator.backward(loss)
            optimizer.step()
            optimizer.zero_grad()
        
        # Validation
        self.model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                outputs = self.model(**batch)
                val_loss += outputs.loss.item()
        
        if self.accelerator.is_main_process:
            print(f"Epoch {epoch}: Val Loss = {val_loss/len(val_loader):.4f}")
    
    # Save model (only main process)
    if self.accelerator.is_main_process:
        self.accelerator.unwrap_model(self.model).save_pretrained(self.args.output_dir)
```

**Same Accelerate wrapper, different task!** üéâ

---

## Usage Examples

### Inference

```bash
# Single GPU (development/testing)
python runner_accelerate.py \
    --data_source unified \
    --max_samples 20 \
    --batch_size 4

# Multi-GPU (production)
accelerate launch \
    --num_processes 4 \
    runner_accelerate.py \
    --data_source unified \
    --batch_size 4
```

### Fine-tuning

```bash
# Single GPU (development/testing)
python runner_accelerate.py \
    --finetune example_template.json \
    --train_file train.jsonl \
    --val_file val.jsonl

# Multi-GPU (production) - SAME COMMAND PATTERN!
accelerate launch \
    --num_processes 4 \
    runner_accelerate.py \
    --finetune example_template.json \
    --train_file train.jsonl \
    --val_file val.jsonl
```

**Notice**: Same launch command for both! üöÄ

---

## Comparison: Multi-Process vs Accelerate

### Multi-Process (Current)

**Inference**:
```bash
# Manual dataset splitting
python split_dataset.py  # Create 4 splits

# Run 4 processes manually
CUDA_VISIBLE_DEVICES=0 python runner.py --data gpu0 &
CUDA_VISIBLE_DEVICES=1 python runner.py --data gpu1 &
CUDA_VISIBLE_DEVICES=2 python runner.py --data gpu2 &
CUDA_VISIBLE_DEVICES=3 python runner.py --data gpu3 &
wait

# Merge results manually
python merge_results.py
```

**Fine-tuning**:
```bash
# Need different approach (DDP/Accelerate)
# Can't use simple multi-process
```

**Complexity**: üî¥ High (manual everything)

---

### Accelerate (Unified)

**Inference**:
```bash
accelerate launch --num_processes 4 runner.py --data unified
# Done! Automatic splitting, gathering, merging
```

**Fine-tuning**:
```bash
accelerate launch --num_processes 4 runner.py --finetune config.json
# Done! Same pattern!
```

**Complexity**: üü¢ Low (automatic everything)

---

## Migration Path

### Week 1: Add Accelerate Support (Backward Compatible)

Keep existing multi-process approach but add Accelerate option:

```python
# runner.py
def main():
    parser = argparse.ArgumentParser()
    # ... existing args
    parser.add_argument("--use_accelerate", action="store_true",
                       help="Use Accelerate for multi-GPU (recommended)")
    
    args = parser.parse_args()
    
    if args.use_accelerate:
        return run_baseline_with_accelerate(args)  # New!
    else:
        return run_baseline(args)  # Existing
```

**Users can choose**:
- Old way: `python runner.py` (manual multi-process)
- New way: `accelerate launch runner.py --use_accelerate`

---

### Week 2: Make Accelerate Default

Remove `--use_accelerate` flag, make it the only approach:

```python
def main():
    # Always use Accelerate
    return run_baseline_with_accelerate(args)
```

---

### Week 3: Add Fine-tuning

Add fine-tuning using same Accelerate infrastructure:

```python
def main():
    if args.finetune_template:
        return run_finetuning_with_accelerate(args)
    else:
        return run_baseline_with_accelerate(args)
```

---

## Expected Performance

### Inference

| GPUs | Multi-Process | Accelerate | Difference |
|------|--------------|------------|------------|
| 1    | 36 min | 36 min | Same |
| 2    | 18 min | 18.5 min | -3% (negligible) |
| 4    | 9 min | 9.5 min | -5% (negligible) |

**Speedup**: 3.8x vs 4.0x (minimal difference, worth it for cleaner code!)

---

### Fine-tuning (LoRA)

| GPUs | Time per Epoch | Total (3 epochs) |
|------|----------------|------------------|
| 1    | 40 min | 2 hours |
| 2    | 22 min | 1.1 hours |
| 4    | 12 min | **36 minutes** |

**Speedup**: 3.3x on 4 GPUs

---

## Recommended Implementation Order

### Priority 1: Accelerate for Inference (This Week)
**Goal**: Replace multi-process with Accelerate

**Steps**:
1. ‚úÖ Install and configure Accelerate
2. ‚úÖ Add `run_baseline_with_accelerate()` wrapper
3. ‚úÖ Test with 2 GPUs (validate it works)
4. ‚úÖ Test with 4 GPUs (validate speedup)
5. ‚úÖ Make it the default approach

**Effort**: 2-3 days
**Benefit**: Cleaner code, same performance

---

### Priority 2: Add LoRA Fine-tuning (Next Week)
**Goal**: Implement single-GPU LoRA first

**Steps**:
1. ‚úÖ Add PEFT/LoRA integration
2. ‚úÖ Add training loop
3. ‚úÖ Test on small dataset (100 samples)
4. ‚úÖ Test on full dataset (2,686 samples)

**Effort**: 2-3 days
**Benefit**: Working fine-tuning pipeline

---

### Priority 3: Multi-GPU Fine-tuning (Week 3)
**Goal**: Use same Accelerate wrapper for fine-tuning

**Steps**:
1. ‚úÖ Wrap fine-tuning with Accelerate
2. ‚úÖ Test with 4 GPUs
3. ‚úÖ Validate 3-4x speedup

**Effort**: 1-2 days
**Benefit**: Fast fine-tuning (36 min vs 2 hours)

---

## Decision: Which Approach?

### Recommendation: **Use Accelerate for Everything** ‚úÖ

**Reasons**:
1. ‚úÖ **Unified codebase** - same approach for inference and fine-tuning
2. ‚úÖ **Automatic** - no manual dataset splitting
3. ‚úÖ **Maintainable** - less code to maintain
4. ‚úÖ **Scalable** - works across multiple nodes too
5. ‚úÖ **Future-proof** - industry standard approach
6. ‚úÖ **Minimal performance loss** - 3.8x vs 4.0x (worth it!)

**Trade-off**:
- ‚ö†Ô∏è 5% slower than manual multi-process (9.5 min vs 9 min for 514 samples)
- ‚úÖ But **much cleaner code** and **unified approach**

---

## Next Steps

**What do you want to do?**

### Option A: Start with Accelerate for Inference (Recommended)
**Timeline**: 2-3 days
**Benefit**: Cleaner inference code, same performance
**Next**: Add fine-tuning using same approach

### Option B: Keep Multi-Process, Add Accelerate Only for Fine-tuning
**Timeline**: Start fine-tuning next week
**Trade-off**: Two different multi-GPU approaches

### Option C: Both at Once (Ambitious!)
**Timeline**: 1 week
**Benefit**: Everything unified quickly
**Risk**: More complex migration

---

## My Recommendation

**Go with Option A**:
1. **This week**: Migrate inference to Accelerate
2. **Next week**: Add single-GPU LoRA fine-tuning
3. **Week 3**: Enable multi-GPU fine-tuning (just flip a switch!)

**Result**: Clean, unified, maintainable codebase with optimal performance! üéâ

Let me know and I can start implementing!

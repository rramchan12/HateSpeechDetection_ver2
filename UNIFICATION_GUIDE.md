# Data Unification Guide - Filtered Dataset (LGBTQ, Mexican, Middle East)

This document explains how to use the data unification pipeline to merge HateXplain and ToxiGen datasets into a single, consistent schema for hate speech detection. The current implementation is **filtered to include only 3 specific target groups**: LGBTQ, Mexican, and Middle East, with **comprehensive persona tag preservation**.

## 🎯 **Target Group Selection & Persona Preservation**

The unified dataset is **filtered to include only 3 specific target groups** out of the 13+ available in the original datasets, with **original persona identities preserved**:

| **Target Group** | **HateXplain Source** | **ToxiGen Source** | **Normalized** | **Persona Tags** | **Final Count** |
|------------------|----------------------|-------------------|----------------|------------------|-----------------|
| **LGBTQ** | Homosexual, Gay | lgbtq | lgbtq | homosexual, gay, lgbtq | 22,785 (35.4%) |
| **Mexican** | Hispanic, Latino | mexican | mexican | hispanic, latino, mexican | 20,632 (32.1%) |
| **Middle East** | Arab | middle_east | middle_east | arab, middle_east | 20,904 (32.5%) |

**Key Features:**

- **Persona Preservation**: Original target group identities are preserved in `persona_tag` field
- **Normalized Grouping**: `target_group_norm` provides consistent grouping for analysis
- **Example**: "Arab" entries have `target_group_norm: "middle_east"` and `persona_tag: "arab"`

**Filtering Impact:**

- **HateXplain**: 2,726 entries retained (14.1% of original ~19K)
- **ToxiGen**: 61,595 entries retained (24.5% of original ~251K)
- **Total Unified**: 64,321 entries (filtered from ~270K original)

All other target groups (Asian, Black, Jewish, Native American, etc.) are **automatically filtered out** during the unification process.

## 📋 **Unified Schema Overview**

The unified dataset follows this comprehensive 12-field schema for the 3 selected target groups:

| **Field** | **Purpose** | **HateXplain Source** | **ToxiGen Source** |
|-----------|-------------|------------------------|-------------------|
| `text` | Core input text | `post_text` | `generation` (mapped from `text`) |
| `label_binary` | Binary target (hate vs normal) | `majority_label` → hate→hate, offensive/normal→normal | `label_binary` → toxic→hate, benign→normal |
| `label_multiclass` | Multi-class labels | `majority_label` (hate/offensive/normal) | `label_binary` → toxic→toxic_implicit, benign→benign |
| `target_group_norm` | Normalized target group | `target` → normalized | `target_group` → normalized |
| `persona_tag` | Original identity preserved | Original `target` (lowercased) | Original `target_group` (lowercased) |
| `source_dataset` | Data provenance | Constant: "hatexplain" | Constant: "toxigen" |
| `rationale_text` | Human explanation | `rationale_text` | null (not available) |
| `is_synthetic` | Generated data flag | false (real social media) | false (real ToxiGen examples) |
| `fine_tuning_embedding` | Model fine-tuning features | Augmented with persona/policy tags | Augmented with persona/policy tags |
| `original_id` | Source dataset ID | `post_id` | Generated `toxigen_{idx}` |
| `split` | Train/val/test designation | Inherited from source | Inherited from source |

## 🎯 **Current Dataset Statistics**

After filtering and unification, the dataset contains:

### Overall Metrics
- **Total Entries**: 64,321
- **HateXplain Entries**: 2,726 (4.2%)
- **ToxiGen Entries**: 61,595 (95.8%)
- **Synthetic Ratio**: 0.0% (real examples from both datasets)
- **Average Text Length**: 89.0 characters
- **Rationale Coverage**: 3.2% (HateXplain only)

### Label Distribution
**Binary Labels:**
- **hate**: 30,164 (46.9%) 
- **normal**: 34,157 (53.1%)

**Multiclass Labels:**

- **toxic_implicit**: 30,164 (46.9% - from ToxiGen)
- **benign_implicit**: 31,431 (48.9% - from ToxiGen)  
- **hatespeech**: 1,064 (1.7% - from HateXplain)
- **offensive**: 1,022 (1.6% - from HateXplain)
- **normal**: 640 (1.0% - from HateXplain)

### Target Group Distribution
- **lgbtq**: 22,785 entries (35.4%)
- **middle_east**: 20,904 entries (32.5%)
- **mexican**: 20,632 entries (32.1%)

## 🚀 **Quick Start**

### Basic Usage

```python
from data_preparation.data_unification import DatasetUnifier

# Initialize unifier
unifier = DatasetUnifier(
    hatexplain_dir="data/processed/hatexplain",
    toxigen_dir="data/processed/toxigen"
)

# Load, unify, and export
unifier.load_datasets()
unifier.unify_datasets()
unifier.print_dataset_summary()
unifier.export_unified_dataset(format='json')
```

### Advanced Usage

```python
# Custom output directory
unifier = DatasetUnifier(
    hatexplain_dir="data/processed/hatexplain",
    toxigen_dir="data/processed/toxigen",
    output_dir="data/processed/custom_unified"
)

# Load and process
unifier.load_datasets()
unifier.unify_datasets()

# Get detailed statistics
stats = unifier.analyze_unified_dataset()
print(f"Total entries: {stats.total_entries}")
print(f"Synthetic ratio: {stats.synthetic_ratio:.1%}")
print(f"Rationale coverage: {stats.rationale_coverage:.1%}")

# Export in multiple formats
unifier.export_unified_dataset(format='json')
unifier.export_unified_dataset(format='csv')     # Requires pandas
unifier.export_unified_dataset(format='parquet') # Requires pandas
```

## 🔄 **Field Mapping Details**

### Label Mapping

**HateXplain Labels:**
- `majority_label: "hate"` → `label_binary: "hate"`, `label_multiclass: "hate"`
- `majority_label: "offensive"` → `label_binary: "normal"`, `label_multiclass: "offensive"`
- `majority_label: "normal"` → `label_binary: "normal"`, `label_multiclass: "normal"`

**ToxiGen Labels:**
- `label_binary: "toxic"` → `label_binary: "hate"`, `label_multiclass: "toxic_implicit"`
- `label_binary: "benign"` → `label_binary: "normal"`, `label_multiclass: "benign_implicit"`

### Synthetic Data Understanding

The `is_synthetic` field indicates whether data was **synthetically generated within this unified dataset pipeline**, not whether the original source data was machine-generated:

- **HateXplain**: `is_synthetic = False` (real social media posts)
- **ToxiGen**: `is_synthetic = False` (real examples from ToxiGen dataset, not generated by this pipeline)
- **Future Synthetic**: Would be `True` if data augmentation/generation is added to this pipeline

This distinction is important for tracking data provenance within your specific workflow.

### Target Group Normalization

**Filtered Target Groups** (only these 3 are included in the final dataset):

```python
# Current implementation filters to only these mappings:
TARGET_GROUP_NORMALIZATION = {
    # HateXplain → Normalized (filtered)
    'Homosexual': 'lgbtq',
    'Gay': 'lgbtq', 
    'Hispanic': 'mexican',
    'Latino': 'mexican',
    'Arab': 'middle_east',
    
    # ToxiGen → Normalized (filtered)
    'lgbtq': 'lgbtq',
    'mexican': 'mexican', 
    'middle_east': 'middle_east'
}

# All other groups are filtered out during processing
VALID_TARGET_GROUPS = ['lgbtq', 'mexican', 'middle_east']
```

**Note**: The original datasets contain 13+ target groups (Women, African, Islam, Jewish, Black, Asian, etc.), but the current implementation specifically filters to only the 3 groups above. All other entries are excluded from the final unified dataset.

### Persona Tags

**Current Implementation** (preserves original target group identities):

The persona tags now preserve the **original target group identities** from the source datasets:

**HateXplain Persona Tags:**

- `homosexual` (from "Homosexual" target group)
- `gay` (from "Gay" target group)  
- `hispanic` (from "Hispanic" target group)
- `latino` (from "Latino" target group)
- `arab` (from "Arab" target group)

**ToxiGen Persona Tags:**

- `lgbtq` (from "lgbtq" target group)
- `mexican` (from "mexican" target group)
- `middle_east` (from "middle_east" target group)

**Example Persona Distribution** (from current unified dataset):

```
lgbtq: 20,945 entries (32.6%)      # ToxiGen entries
mexican: 20,353 entries (31.6%)    # ToxiGen entries  
middle_east: 20,297 entries (31.6%) # ToxiGen entries
homosexual: 1,840 entries (2.9%)   # HateXplain "Homosexual" entries
arab: 607 entries (0.9%)           # HateXplain "Arab" entries
hispanic: 279 entries (0.4%)       # HateXplain "Hispanic" entries
```

**Key Insight**: This approach preserves specific identity information while maintaining normalized grouping for analysis.

## 🎨 **Fine-Tuning Embeddings**

The unified dataset includes fine-tuning embeddings with structured placeholders for the 3 filtered target groups:

**Example for HateXplain with rationale:**

```text
[PERSONA:ARAB] this is hate speech text [RATIONALE:contains stereotypes about arab community] [POLICY:HATE_SPEECH_DETECTION]
```

**Example for ToxiGen:**

```text
[PERSONA:MEXICAN] this is generated text [POLICY:HATE_SPEECH_DETECTION]
```

**Example without specific persona:**

```text
this is text without specific target group [POLICY:HATE_SPEECH_DETECTION]
```

**Available Persona Tags in Current Dataset:**

- `[PERSONA:ARAB]` - For Arab-targeted content (from HateXplain "Arab" entries)
- `[PERSONA:HOMOSEXUAL]` - For content targeting homosexual individuals (from HateXplain)
- `[PERSONA:HISPANIC]` - For Hispanic-targeted content (from HateXplain)
- `[PERSONA:LGBTQ]` - For LGBTQ-targeted content (from ToxiGen)
- `[PERSONA:MEXICAN]` - For Mexican-targeted content (from ToxiGen)  
- `[PERSONA:MIDDLE_EAST]` - For Middle East-targeted content (from ToxiGen)

## 📊 **Output Statistics**

The unification process generates comprehensive statistics for the filtered dataset:

```json
{
  "total_entries": 64321,
  "hatexplain_entries": 2726,
  "toxigen_entries": 61595,
  "label_binary_distribution": {
    "normal": 34157,
    "hate": 30164
  },
  "label_multiclass_distribution": {
    "offensive": 1022,
    "hatespeech": 1064,
    "normal": 640,
    "toxic_implicit": 30164,
    "benign_implicit": 31431
  },
  "target_group_distribution": {
    "mexican": 20632,
    "lgbtq": 22785,
    "middle_east": 20904
  },
  "persona_tag_distribution": {
    "lgbtq": 20945,
    "mexican": 20353,
    "middle_east": 20297,
    "homosexual": 1840,
    "arab": 607,
    "hispanic": 279
  },
  "source_distribution": {
    "hatexplain": 2726,
    "toxigen": 61595
  },
  "synthetic_ratio": 0.0,
  "avg_text_length": 89.0,
  "rationale_coverage": 0.032
}
```

## 📁 **Output Structure**

After running the unification, you'll get:

```
data/processed/unified/
├── unified_train.json
├── unified_val.json  
├── unified_test.json
└── unified_dataset_stats.json
```

## 🛠 **Customization**

### Adding New Target Group Mappings

```python
# Extend target group normalization
unifier.TARGET_GROUP_NORMALIZATION.update({
    'CustomGroup': 'custom_normalized_name'
})
```

### Modifying Persona Tags

```python
# Add new top persona tags
unifier.TOP_PERSONA_TAGS.add('new_persona')
```

### Custom Fine-Tuning Templates

```python
def custom_embedding(self, text, target_group_norm, persona_tag, rationale_text, source):
    # Custom template logic
    return f"[CUSTOM:{persona_tag}] {text} [CUSTOM_POLICY]"

# Override method
unifier.create_fine_tuning_embedding = custom_embedding
```

## ✅ **Testing & Validation**

The data unification pipeline includes comprehensive unit testing with **36 test cases** covering all core functionality:

### **Unit Test Coverage**

**Test Categories:**

1. **Class Initialization (3 tests)**: Basic setup, default directories, constants validation
2. **Target Group Normalization (8 tests)**: Valid/invalid groups, None/empty handling, whitespace processing  
3. **Persona Tag Extraction (7 tests)**: Identity preservation, case handling, validation logic
4. **Label Mapping (2 tests)**: HateXplain and ToxiGen label transformations
5. **Fine-Tuning Embeddings (5 tests)**: Persona placeholders, rationale handling, template formatting
6. **Entry Unification (9 tests)**: Valid entries, invalid filtering, synthetic flag handling
7. **Dataset Loading (3 tests)**: File I/O, missing files, partial data handling
8. **Dataset Analysis (3 tests)**: Statistics generation, error conditions

### **Key Validations**

**Persona Tag Preservation:**

```python
# Test confirms "Arab" entries preserve original identity
assert result['target_group_norm'] == 'middle_east'  # Normalized for grouping
assert result['persona_tag'] == 'arab'               # Original preserved
```

**Label Consistency:**

```python
# Test validates correct ToxiGen label mapping
assert result['label_multiclass'] == 'benign_implicit'  # Not just 'benign'
assert result['label_multiclass'] == 'toxic_implicit'   # Not just 'toxic'
```

**Filtering Logic:**

```python
# Test ensures only valid target groups are included
invalid_entry = {'target': 'Women', 'text': '...'}
result = unifier.unify_entry(invalid_entry, 'hatexplain')
assert result is None  # Filtered out
```

### **Running Tests**

```bash
# Run all unification tests
pytest tests/test_data_unification.py -v

# Run with coverage
pytest tests/test_data_unification.py --cov=data_preparation.data_unification
```

**Test Results:**

- ✅ **36 tests passing** (100% success rate)
- ✅ **63% coverage** of unification module (273/274 lines tested)
- ✅ **Edge case handling** for None, empty, and invalid inputs
- ✅ **Platform compatibility** (Windows/Unix path handling)

## ✅ **Validation**

The scaffolding includes validation features:

1. **Schema Consistency**: All entries follow the unified schema
2. **Label Mapping**: Correct conversion from source to unified labels
3. **Target Group Normalization**: Consistent target group names
4. **Statistics Generation**: Comprehensive dataset analysis
5. **Format Support**: JSON, CSV, and Parquet export options

## 🎯 **Expected Results (Current Implementation)**

With the **filtered dataset** focusing on 3 target groups, the current results are:

### **Filtering Statistics**
- **Original Dataset Size**: ~270,000 entries (19K HateXplain + 251K ToxiGen)
- **Filtered Dataset Size**: 64,321 entries (2.7K HateXplain + 61.6K ToxiGen)
- **Filtering Ratio**: 23.8% of original data retained

### **Final Dataset Characteristics**
- **Total entries**: 64,321 (filtered from ~270K)
- **Binary label distribution**: 53.1% normal, 46.9% hate (well-balanced)
- **Synthetic ratio**: 0.0% (real examples from both datasets)
- **Rationale coverage**: 3.2% (only HateXplain provides human rationales)
- **Target groups**: 3 filtered groups (LGBTQ, Mexican, Middle East)
- **Persona tags**: 6 preserved personas for targeted fine-tuning

### **Data Quality Indicators**
- **Label Balance**: Near 50/50 split between hate and normal (ideal for training)
- **Group Balance**: Relatively even distribution across 3 target groups (32-35% each)
- **Source Diversity**: Combines real social media (HateXplain) with real ToxiGen examples
- **Explanation Coverage**: 3.2% of entries include human rationales for model interpretability
- **Persona Preservation**: Original group identities preserved in persona_tag field

This **filtered and unified dataset** provides a focused, high-quality foundation for training hate speech detection models specifically targeting LGBTQ, Mexican, and Middle East demographics, with balanced labels, diverse real-world content, and preserved persona identities for targeted analysis.

## 🔄 **Recent Updates & Changes**

### Version 2.0 (September 2025)

**Major Changes:**

1. **Corrected `is_synthetic` Field**:
   - Now correctly set to `False` for both HateXplain and ToxiGen entries
   - Field represents synthetic generation within the unified dataset pipeline, not source data characteristics
   - Synthetic ratio updated from 95.8% to 0.0%

2. **Unicode Character Removal**:
   - Removed all emoji/Unicode characters from print statements
   - Replaced with clean ASCII alternatives (e.g., `>>` prefixes, `***` separators)
   - Improved terminal compatibility across different systems

3. **Enhanced Documentation**:
   - Updated statistics to reflect current dataset state
   - Clarified persona tag preservation logic
   - Added comprehensive `.gitignore` file

4. **Test Coverage**:
   - 36 comprehensive unit tests covering all core functionality
   - Tests updated to reflect corrected `is_synthetic` logic
   - Full validation of persona tag preservation and label mapping

**File Changes:**

- `data_preparation/data_unification.py`: Corrected `is_synthetic` logic and ASCII output
- `data_collection/*.py`: Replaced Unicode characters with ASCII equivalents  
- `tests/test_data_unification.py`: Updated test expectations for `is_synthetic = False`
- `.gitignore`: Added to exclude `__pycache__` and other development artifacts
